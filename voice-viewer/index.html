<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DAL Voice Assistant</title>

  <style>
    :root {
      --wrap: 900px;
      --pad: 16px;
      --border: #e5e7eb;
      --accent: #f0c44c;   /* Dal gold */
      --text: #ffffff;     /* white text */
      --bg: #0c2d5a;       /* Dal navy */
      --bg-page: #0c2d5a;  /* background navy */
      --muted: #dbe7ff;
      --link: #9cc9ff;
      --link-visited: #c8e0ff;
    }

    html, body {
      margin: 0;
      padding: 0;
      background: var(--bg-page);
      color: var(--text);
      font: 16px/1.5 system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;
    }

    .wrap {
      max-width: var(--wrap);
      margin: 0 auto;
      padding: var(--pad);
      text-align: center;
    }

    h2 {
      margin-top: 12px;
      color: var(--accent);
    }

    button {
      background: var(--accent);
      color: #000;
      border: none;
      border-radius: 10px;
      padding: 14px 22px;
      font-size: 18px;
      font-weight: bold;
      cursor: pointer;
      margin-top: 20px;
      transition: background 0.2s ease;
    }

    button:hover { background: #ffd84a; }
    button:disabled { opacity: 0.6; cursor: not-allowed; }

    #out {
      margin-top: 28px;
      text-align: left;
      line-height: 1.6;
    }

    a { color: var(--link); }
    a:visited { color: var(--link-visited); }

    .error { color: #ffb8b8; font-weight: bold; }
  </style>

  <!-- Markdown + Sanitizer -->
  <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/dompurify@3.1.6/dist/purify.min.js"></script>
</head>

<body>
  <div class="wrap">
    <h2>üéôÔ∏è DAL Voice Assistant</h2>
    <p>Speak naturally ‚Äî interrupt anytime for clarification.</p>
    <button id="micBtn">üé§ Press to Speak</button>
    <div id="out"><em>Waiting for input‚Ä¶</em></div>

    <!-- üîä NEW: Audio element for realistic OpenAI voice playback -->
    <audio id="voicePlayer" autoplay></audio>
  </div>

  <script>
    const backendURL = "https://dalbackendproduction-production.up.railway.app/ask";
    // üîä NEW: voice route (handled safely in backend using OPENAI_API_KEY_DAL)
    const voiceBackend = "https://dalbackendproduction-production.up.railway.app/voice";

    const micBtn = document.getElementById("micBtn");
    const outEl = document.getElementById("out");
    const audioEl = document.getElementById("voicePlayer");

    let recognition;
    let recognizing = false;
    let voices = [];
    let autoListen = false;

    // Load browser fallback voices (kept intact)
    window.speechSynthesis.onvoiceschanged = () => {
      voices = window.speechSynthesis.getVoices();
    };

    // ===============================
    // üß† Original browser TTS fallback
    // ===============================
    function speak(text, cb) {
      if (!text) return;
      const utter = new SpeechSynthesisUtterance(stripTags(text));
      utter.lang = "en-CA";
      utter.rate = 1.0;
      utter.pitch = 1.0;

      const preferredVoices = [
        "Google UK English Female",
        "Google US English",
        "Samantha",
        "Karen",
        "Daniel",
        "Alex"
      ];
      const available = window.speechSynthesis.getVoices();
      const match =
        available.find(v => preferredVoices.includes(v.name)) ||
        available.find(v => v.lang.startsWith("en")) ||
        available[0];

      if (match) utter.voice = match;
      utter.onend = () => { if (cb) cb(); };

      window.speechSynthesis.cancel();
      window.speechSynthesis.speak(utter);
    }

    // Strip HTML tags before reading aloud
    function stripTags(text) {
      const tmp = document.createElement("div");
      tmp.innerHTML = text;
      return tmp.textContent || tmp.innerText || "";
    }

    // ===============================
    // üé§ Speech Recognition
    // ===============================
    function startRecognition() {
      if (!("webkitSpeechRecognition" in window)) {
        alert("Speech recognition not supported. Please use Chrome.");
        return;
      }

      recognition = new webkitSpeechRecognition();
      recognition.lang = "en-CA";
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      recognition.onstart = () => {
        recognizing = true;
        micBtn.textContent = "üéß Listening...";
        outEl.innerHTML = "<em>Listening...</em>";
        window.speechSynthesis.cancel(); // stop current voice if speaking
        audioEl.pause();
      };

      recognition.onerror = e => {
        recognizing = false;
        micBtn.textContent = "üé§ Press to Speak";
        outEl.innerHTML = `<span class="error">Error: ${e.error}</span>`;
      };

      recognition.onresult = e => {
        recognizing = false;
        micBtn.textContent = "üé§ Press to Speak";
        const transcript = e.results[0][0].transcript;
        outEl.innerHTML = `<b>You said:</b> ${transcript}<br><em>Processing...</em>`;
        sendToBackend(transcript);
      };

      recognition.onend = () => {
        recognizing = false;
        micBtn.textContent = "üé§ Press to Speak";
        if (autoListen) setTimeout(startRecognition, 400); // keep conversation open
      };

      recognition.start();
    }

    // Handle button press
    micBtn.addEventListener("click", () => {
      if (recognizing) {
        recognition.stop();
        micBtn.textContent = "üé§ Press to Speak";
        autoListen = false;
        return;
      }
      autoListen = true;
      startRecognition();
    });

    // ===============================
    // üîó Backend: send user question
    // ===============================
    async function sendToBackend(question) {
      try {
        const payload = { question };
        const res = await fetch(backendURL, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(payload)
        });

        const data = await res.json();
        const answer = data.answer || "[No answer received]";
        const html = DOMPurify.sanitize(marked.parse(answer));
        outEl.innerHTML = `<b>Assistant:</b><br>${html}`;

        // üîä NEW: Prefer OpenAI ‚Äúverse‚Äù voice if available
        playOpenAIVoice(answer, () => {
          if (autoListen) startRecognition();
        });

      } catch (err) {
        outEl.innerHTML = `<span class="error">Backend error: ${err.message}</span>`;
      }
    }

    // ===============================
    // üîä NEW: Request OpenAI voice via DAL backend
    // ===============================
    async function playOpenAIVoice(text, cb) {
      try {
        const response = await fetch(voiceBackend, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text, voice: "verse" })
        });

        if (!response.ok) throw new Error("Failed to generate voice");

        const audioBlob = await response.blob();
        const audioURL = URL.createObjectURL(audioBlob);
        audioEl.src = audioURL;

        audioEl.onended = () => {
          if (cb) cb();
        };
      } catch (err) {
        console.error("Voice generation error:", err);
        // fallback to browser voice if OpenAI fails
        speak(text, cb);
      }
    }

    // ===============================
    // üéô Ask for mic access early
    // ===============================
    navigator.mediaDevices.getUserMedia({ audio: true }).catch(() => {
      outEl.innerHTML =
        `<span class="error">Microphone permission denied. Please enable it in browser settings.</span>`;
    });
  </script>
</body>
</html>
